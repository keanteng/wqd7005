{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bde8b69",
   "metadata": {},
   "source": [
    "# Data Profiling & Validation Using LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0e1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure api\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86f5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "model = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7800e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'CustomerID': [1001, 1002, 1003, None, 1005],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Email': ['alice@gmail', 'bob@yahoo.com', 'charlie@@mail.com', None, 'eve@gmail.com'],\n",
    "    'JoinDate': ['2021-01-01', '2021-02-30', '2021-03-15', 'bad_date', '2021/04/01'],\n",
    "    'Country': ['MY', 'Malaysia', 'MY', 'Singapore', None],\n",
    "    'Revenue': ['1000', 'Two Thousand', '3000', '-500', '4000']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert schema to prompt\n",
    "schema = str(df.dtypes)\n",
    "prompt = f\"\"\"\n",
    "You are a data quality expert. Analyze the schema below and:\n",
    "1. Identify 3 potential data quality issues.\n",
    "2. Suggest data profiling steps.\n",
    "3. Recommend data standardization or validation actions.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bfad7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's analyze the provided schema and address the data quality concerns.\n",
      "\n",
      "**1. Potential Data Quality Issues**\n",
      "\n",
      "Here are three potential data quality issues, along with explanations:\n",
      "\n",
      "*   **a. `CustomerID` as Float:**  A `CustomerID` should ideally be a unique identifier and is usually represented as an integer or a string.  Using `float64` can lead to problems:\n",
      "    *   **Loss of precision:** Floating-point numbers can have rounding errors, which is unacceptable for IDs.  e.g., You might expect 12345 but get 12345.0000000000001.\n",
      "    *   **Inconsistency:**  Having `CustomerID` as a float suggests potentially meaningless decimal places. What does CustomerID 123.5 represent?\n",
      "    *   **Joining Issues:** Joining with other tables where CustomerID is an integer or string will be problematic without explicit type casting.\n",
      "\n",
      "*   **b. `JoinDate` as Object:**  Storing dates as generic `object` types (usually strings) is a major data quality issue.  It makes date-based analysis and calculations incredibly difficult and error-prone.\n",
      "    *   **Format Inconsistencies:**  The `JoinDate` might have various formats like \"YYYY-MM-DD\", \"MM/DD/YYYY\", \"DD-MMM-YY\", etc.\n",
      "    *   **Sorting/Filtering Problems:** Sorting strings lexicographically will not give the correct chronological order.  Filtering by date ranges will also be incorrect.\n",
      "    *   **Calculations Impossible:**  Calculating the age of customers or the duration of their membership is impossible without parsing these strings into proper date objects.\n",
      "\n",
      "*   **c. `Revenue` as Object:** Revenue should be a numerical type (either integer or float). Storing it as `object` (most likely a string) is a serious issue.\n",
      "    *   **Invalid Characters:** The `Revenue` strings might contain currency symbols (\"$\", \"€\"), commas (\",\"), or non-numeric characters.\n",
      "    *   **Incorrect Aggregations:** You cannot perform sums, averages, or other calculations directly on string data.\n",
      "    *   **Potential Missing Values:** There might be \"NA\" \"None\" or empty strings being stored.\n",
      "\n",
      "**2. Data Profiling Steps**\n",
      "\n",
      "Data profiling is essential to understand the *actual* data characteristics before attempting cleaning or transformation. Here's a suggested set of profiling steps:\n",
      "\n",
      "*   **a. Descriptive Statistics:**\n",
      "    *   For *all* columns:\n",
      "        *   Count of non-null values.\n",
      "        *   Number of unique values.\n",
      "        *   Number of missing (null) values.\n",
      "        *   Data type confirmation.\n",
      "\n",
      "    *   For `CustomerID`:\n",
      "        *   Minimum, Maximum (to get the range of IDs)\n",
      "        *   Check for duplicate CustomerIDs\n",
      "\n",
      "    *   For `Name`:\n",
      "        *   Most frequent names.\n",
      "        *   Average name length.\n",
      "\n",
      "    *   For `Email`:\n",
      "        *   Percentage of valid email formats (using regex).\n",
      "        *   Most frequent domain names.\n",
      "        *   Number of duplicate emails.\n",
      "\n",
      "    *   For `JoinDate`:\n",
      "        *   Most frequent date formats.\n",
      "        *   Minimum and maximum dates (to identify outliers or impossible dates).\n",
      "        *   Frequency of dates.\n",
      "        *   Identify any invalid date entries like \"N/A\"\n",
      "\n",
      "    *   For `Country`:\n",
      "        *   List of unique country values.\n",
      "        *   Value counts for each country (to identify potential data entry errors or unexpected distributions).\n",
      "\n",
      "    *   For `Revenue`:\n",
      "        *   Identify values with non-numeric characters (currency symbols, commas, etc.).\n",
      "        *   Identify potential missing values (e.g., \"NA\", \"\", \"None\").\n",
      "\n",
      "*   **b. Value Distribution Analysis:**\n",
      "    *   Histograms or bar charts for numeric/categorical columns to visualize distributions.\n",
      "    *   Frequency tables to show the occurrences of each unique value in categorical columns.\n",
      "\n",
      "*   **c. Pattern Analysis:**\n",
      "    *   Use regular expressions to identify patterns in `Email`, `Name`, `JoinDate`, and `Revenue` to understand common formats and potential anomalies.  For example, check for common email patterns (e.g., `^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$`).\n",
      "\n",
      "*   **d. Dependency Analysis:**\n",
      "    *   Are there any relationships between columns? For example, are certain countries associated with specific revenue ranges or email domains?\n",
      "\n",
      "**3. Data Standardization and Validation Actions**\n",
      "\n",
      "Based on the identified issues and the insights gained from data profiling, here are recommended actions:\n",
      "\n",
      "*   **a. `CustomerID`:**\n",
      "    *   **Data Type Conversion:**  Convert `CustomerID` to `int64` if all values are integers and there are no missing values. If there are any non-integer values, investigate and either convert to integer or string.\n",
      "    *   **Validation:** Ensure `CustomerID` values are unique.  Identify and handle duplicates (e.g., merge records, assign new IDs).\n",
      "    *   **Format Standardization:** If the IDs have a specific format (e.g., prefixed with \"CUST-\"), enforce that format.\n",
      "\n",
      "*   **b. `Name`:**\n",
      "    *   **Standardization:** Remove leading/trailing whitespace.\n",
      "    *   **Case Consistency:** Convert to a consistent case (e.g., title case, lowercase).\n",
      "    *   **Splitting:** Consider splitting the `Name` column into `FirstName` and `LastName` if appropriate.\n",
      "    *   **Validation:**  Check for and handle special characters or unexpected symbols.\n",
      "\n",
      "*   **c. `Email`:**\n",
      "    *   **Validation:** Use regular expressions to validate email format.  Flag or correct invalid emails.\n",
      "    *   **Domain Standardization:** Standardize domain names (e.g., convert \"gmail.com\" to \"google.com\" if necessary).\n",
      "    *   **Deduplication:** Identify and handle duplicate email addresses.\n",
      "    *   **Consider Email Verification:** Use a third-party service to verify email addresses if real-time validation is important.\n",
      "\n",
      "*   **d. `JoinDate`:**\n",
      "    *   **Data Type Conversion:** Convert `JoinDate` to a datetime data type (e.g., `datetime64[ns]` in pandas).\n",
      "    *   **Format Standardization:** Identify the dominant date format and use `pd.to_datetime(df['JoinDate'], format='%Y-%m-%d', errors='coerce')` to parse the column. The `errors='coerce'` option handles any dates that do not conform to the specified format and turn them to 'NaT'\n",
      "    *   **Missing Value Handling:** Decide how to handle invalid dates (e.g., replace with a default date, impute based on other data).\n",
      "    *   **Range Validation:** Check for dates that are illogical (e.g., future dates or dates that are historically impossible).\n",
      "\n",
      "*   **e. `Country`:**\n",
      "    *   **Standardization:**  Use a standard list of country codes (e.g., ISO 3166-1 alpha-2 or alpha-3) and map inconsistent names to the standard codes.  Remove leading/trailing whitespace.  Handle variations in spelling (e.g., \"USA\" vs. \"United States of America\").\n",
      "    *   **Validation:**  Validate that the country values are valid according to the standard list.\n",
      "\n",
      "*   **f. `Revenue`:**\n",
      "    *   **Data Cleaning:** Remove currency symbols (e.g., \"$\", \"€\"), commas, and other non-numeric characters using regex and the `.replace` function.\n",
      "    *   **Data Type Conversion:** Convert `Revenue` to a numeric data type (e.g., `float64` or `int64`).\n",
      "    *   **Missing Value Handling:** Decide how to handle missing or invalid revenue values (e.g., replace with 0, calculate an average, or flag the record).\n",
      "    *   **Range Validation:** Check for revenue values that are illogical (e.g., negative values if not allowed, or very high outliers).\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Data Source Understanding:** Understand the origin of the data and the business rules associated with each field.\n",
      "*   **Imputation vs. Removal:** Carefully consider whether to impute missing values or remove records with missing values. Imputation can introduce bias, while removal can reduce the sample size.\n",
      "*   **Data Quality Monitoring:** Implement a process for ongoing data quality monitoring to detect and address issues as they arise.\n",
      "*   **Document Everything:**  Document all data cleaning and transformation steps to ensure reproducibility and maintainability.\n",
      "*   **Testing:** Test any changes that are made to the data. Make sure you can revert your steps if you need to.\n",
      "\n",
      "By following these steps, you can significantly improve the quality and reliability of your data. Remember to tailor these recommendations to the specific characteristics of your data and business requirements.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: Generate and Print the Output \n",
    "response = client.models.generate_content(model=model, contents=prompt)\n",
    "print(response.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f43a3d",
   "metadata": {},
   "source": [
    "## Model Output:\n",
    "\n",
    "Okay, let's analyze the provided schema and address the data quality concerns.\n",
    "\n",
    "**1. Potential Data Quality Issues**\n",
    "\n",
    "Here are three potential data quality issues, along with explanations:\n",
    "\n",
    "*   **a. `CustomerID` as Float:**  A `CustomerID` should ideally be a unique identifier and is usually represented as an integer or a string.  Using `float64` can lead to problems:\n",
    "    *   **Loss of precision:** Floating-point numbers can have rounding errors, which is unacceptable for IDs.  e.g., You might expect 12345 but get 12345.0000000000001.\n",
    "    *   **Inconsistency:**  Having `CustomerID` as a float suggests potentially meaningless decimal places. What does CustomerID 123.5 represent?\n",
    "    *   **Joining Issues:** Joining with other tables where CustomerID is an integer or string will be problematic without explicit type casting.\n",
    "\n",
    "*   **b. `JoinDate` as Object:**  Storing dates as generic `object` types (usually strings) is a major data quality issue.  It makes date-based analysis and calculations incredibly difficult and error-prone.\n",
    "    *   **Format Inconsistencies:**  The `JoinDate` might have various formats like \"YYYY-MM-DD\", \"MM/DD/YYYY\", \"DD-MMM-YY\", etc.\n",
    "    *   **Sorting/Filtering Problems:** Sorting strings lexicographically will not give the correct chronological order.  Filtering by date ranges will also be incorrect.\n",
    "    *   **Calculations Impossible:**  Calculating the age of customers or the duration of their membership is impossible without parsing these strings into proper date objects.\n",
    "\n",
    "*   **c. `Revenue` as Object:** Revenue should be a numerical type (either integer or float). Storing it as `object` (most likely a string) is a serious issue.\n",
    "    *   **Invalid Characters:** The `Revenue` strings might contain currency symbols (\"$\", \"€\"), commas (\",\"), or non-numeric characters.\n",
    "    *   **Incorrect Aggregations:** You cannot perform sums, averages, or other calculations directly on string data.\n",
    "    *   **Potential Missing Values:** There might be \"NA\" \"None\" or empty strings being stored.\n",
    "\n",
    "**2. Data Profiling Steps**\n",
    "\n",
    "Data profiling is essential to understand the *actual* data characteristics before attempting cleaning or transformation. Here's a suggested set of profiling steps:\n",
    "\n",
    "*   **a. Descriptive Statistics:**\n",
    "    *   For *all* columns:\n",
    "        *   Count of non-null values.\n",
    "        *   Number of unique values.\n",
    "        *   Number of missing (null) values.\n",
    "        *   Data type confirmation.\n",
    "\n",
    "    *   For `CustomerID`:\n",
    "        *   Minimum, Maximum (to get the range of IDs)\n",
    "        *   Check for duplicate CustomerIDs\n",
    "\n",
    "    *   For `Name`:\n",
    "        *   Most frequent names.\n",
    "        *   Average name length.\n",
    "\n",
    "    *   For `Email`:\n",
    "        *   Percentage of valid email formats (using regex).\n",
    "        *   Most frequent domain names.\n",
    "        *   Number of duplicate emails.\n",
    "\n",
    "    *   For `JoinDate`:\n",
    "        *   Most frequent date formats.\n",
    "        *   Minimum and maximum dates (to identify outliers or impossible dates).\n",
    "        *   Frequency of dates.\n",
    "        *   Identify any invalid date entries like \"N/A\"\n",
    "\n",
    "    *   For `Country`:\n",
    "        *   List of unique country values.\n",
    "        *   Value counts for each country (to identify potential data entry errors or unexpected distributions).\n",
    "\n",
    "    *   For `Revenue`:\n",
    "        *   Identify values with non-numeric characters (currency symbols, commas, etc.).\n",
    "        *   Identify potential missing values (e.g., \"NA\", \"\", \"None\").\n",
    "\n",
    "*   **b. Value Distribution Analysis:**\n",
    "    *   Histograms or bar charts for numeric/categorical columns to visualize distributions.\n",
    "    *   Frequency tables to show the occurrences of each unique value in categorical columns.\n",
    "\n",
    "*   **c. Pattern Analysis:**\n",
    "    *   Use regular expressions to identify patterns in `Email`, `Name`, `JoinDate`, and `Revenue` to understand common formats and potential anomalies.  For example, check for common email patterns (e.g., `^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$`).\n",
    "\n",
    "*   **d. Dependency Analysis:**\n",
    "    *   Are there any relationships between columns? For example, are certain countries associated with specific revenue ranges or email domains?\n",
    "\n",
    "**3. Data Standardization and Validation Actions**\n",
    "\n",
    "Based on the identified issues and the insights gained from data profiling, here are recommended actions:\n",
    "\n",
    "*   **a. `CustomerID`:**\n",
    "    *   **Data Type Conversion:**  Convert `CustomerID` to `int64` if all values are integers and there are no missing values. If there are any non-integer values, investigate and either convert to integer or string.\n",
    "    *   **Validation:** Ensure `CustomerID` values are unique.  Identify and handle duplicates (e.g., merge records, assign new IDs).\n",
    "    *   **Format Standardization:** If the IDs have a specific format (e.g., prefixed with \"CUST-\"), enforce that format.\n",
    "\n",
    "*   **b. `Name`:**\n",
    "    *   **Standardization:** Remove leading/trailing whitespace.\n",
    "    *   **Case Consistency:** Convert to a consistent case (e.g., title case, lowercase).\n",
    "    *   **Splitting:** Consider splitting the `Name` column into `FirstName` and `LastName` if appropriate.\n",
    "    *   **Validation:**  Check for and handle special characters or unexpected symbols.\n",
    "\n",
    "*   **c. `Email`:**\n",
    "    *   **Validation:** Use regular expressions to validate email format.  Flag or correct invalid emails.\n",
    "    *   **Domain Standardization:** Standardize domain names (e.g., convert \"gmail.com\" to \"google.com\" if necessary).\n",
    "    *   **Deduplication:** Identify and handle duplicate email addresses.\n",
    "    *   **Consider Email Verification:** Use a third-party service to verify email addresses if real-time validation is important.\n",
    "\n",
    "*   **d. `JoinDate`:**\n",
    "    *   **Data Type Conversion:** Convert `JoinDate` to a datetime data type (e.g., `datetime64[ns]` in pandas).\n",
    "    *   **Format Standardization:** Identify the dominant date format and use `pd.to_datetime(df['JoinDate'], format='%Y-%m-%d', errors='coerce')` to parse the column. The `errors='coerce'` option handles any dates that do not conform to the specified format and turn them to 'NaT'\n",
    "    *   **Missing Value Handling:** Decide how to handle invalid dates (e.g., replace with a default date, impute based on other data).\n",
    "    *   **Range Validation:** Check for dates that are illogical (e.g., future dates or dates that are historically impossible).\n",
    "\n",
    "*   **e. `Country`:**\n",
    "    *   **Standardization:**  Use a standard list of country codes (e.g., ISO 3166-1 alpha-2 or alpha-3) and map inconsistent names to the standard codes.  Remove leading/trailing whitespace.  Handle variations in spelling (e.g., \"USA\" vs. \"United States of America\").\n",
    "    *   **Validation:**  Validate that the country values are valid according to the standard list.\n",
    "\n",
    "*   **f. `Revenue`:**\n",
    "    *   **Data Cleaning:** Remove currency symbols (e.g., \"$\", \"€\"), commas, and other non-numeric characters using regex and the `.replace` function.\n",
    "    *   **Data Type Conversion:** Convert `Revenue` to a numeric data type (e.g., `float64` or `int64`).\n",
    "    *   **Missing Value Handling:** Decide how to handle missing or invalid revenue values (e.g., replace with 0, calculate an average, or flag the record).\n",
    "    *   **Range Validation:** Check for revenue values that are illogical (e.g., negative values if not allowed, or very high outliers).\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "*   **Data Source Understanding:** Understand the origin of the data and the business rules associated with each field.\n",
    "*   **Imputation vs. Removal:** Carefully consider whether to impute missing values or remove records with missing values. Imputation can introduce bias, while removal can reduce the sample size.\n",
    "*   **Data Quality Monitoring:** Implement a process for ongoing data quality monitoring to detect and address issues as they arise.\n",
    "*   **Document Everything:**  Document all data cleaning and transformation steps to ensure reproducibility and maintainability.\n",
    "*   **Testing:** Test any changes that are made to the data. Make sure you can revert your steps if you need to.\n",
    "\n",
    "By following these steps, you can significantly improve the quality and reliability of your data. Remember to tailor these recommendations to the specific characteristics of your data and business requirements.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
